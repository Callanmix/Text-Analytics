{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dfply import *\n",
    "\n",
    "# Formatting Text\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Text Analysis\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "from gensim.summarization.summarizer import summarize \n",
    "from gensim.summarization import keywords \n",
    "from spacy import displacy\n",
    "import textacy\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "import networkx as nx\n",
    "\n",
    "# Sentiment Analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# WordCloud\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "import urllib\n",
    "import requests\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_csv(\"GenConfTalks.csv\")\n",
    "frame.fillna(\"NA\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Date = []\n",
    "for m, y in zip(frame.Month, frame.Year):\n",
    "    Date.append(date(y, m, 1))\n",
    "frame['Date'] = Date\n",
    "\n",
    "grouped_ = frame.groupby(['Date']).count()\n",
    "grouped_.insert(0, 'ID', range(100, 100 + len(grouped_)))\n",
    "frame.set_index(['Date'], inplace=True)\n",
    "\n",
    "frame = pd.merge(frame, grouped_[['ID']], right_on='Date', left_index=True)\n",
    "frame.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = list(set(stopwords.words(\"english\"))) + list([x for x in string.punctuation])\n",
    "printable = set(string.printable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokening(text): \n",
    "    token = word_tokenize(text)\n",
    "    token = [word.lower() for word in token if word not in stop]\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Role</th>\n",
       "      <th>Title</th>\n",
       "      <th>Talk</th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>Ulisses Soares</td>\n",
       "      <td>Of the Quorum of the Twelve Apostles</td>\n",
       "      <td>How Can I Understand?</td>\n",
       "      <td>My dear brothers and sisters, what a great joy...</td>\n",
       "      <td>196</td>\n",
       "      <td>[My dear brothers and sisters, what a great jo...</td>\n",
       "      <td>[my, dear, brothers, sisters, great, joy, toge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>Becky Craven</td>\n",
       "      <td>Second Counselor in the Young Women General Pr...</td>\n",
       "      <td>Careful versus Casual</td>\n",
       "      <td>I once saw a sign in a store window that said,...</td>\n",
       "      <td>196</td>\n",
       "      <td>[I once saw a sign in a store window that said...</td>\n",
       "      <td>[i, saw, sign, store, window, said, happiness,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>Brook P. Hales</td>\n",
       "      <td>Of the Seventy</td>\n",
       "      <td>Answers to Prayer</td>\n",
       "      <td>An important and comforting doctrine of the go...</td>\n",
       "      <td>196</td>\n",
       "      <td>[An important and comforting doctrine of the g...</td>\n",
       "      <td>[an, important, comforting, doctrine, gospel, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>Dieter F. Uchtdorf</td>\n",
       "      <td>Of the Quorum of the Twelve Apostles</td>\n",
       "      <td>Missionary Work: Sharing What Is in Your Heart</td>\n",
       "      <td>Last month the Twelve were invited by our dear...</td>\n",
       "      <td>196</td>\n",
       "      <td>[Last month the Twelve were invited by our dea...</td>\n",
       "      <td>[last, month, twelve, invited, dear, prophet, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-04-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>W. Christopher Waddell</td>\n",
       "      <td>Second Counselor in the Presiding Bishopric</td>\n",
       "      <td>Just as He Did</td>\n",
       "      <td>Approximately 18 months ago, in the fall of 20...</td>\n",
       "      <td>196</td>\n",
       "      <td>[Approximately 18 months ago, in the fall of 2...</td>\n",
       "      <td>[approximately, 18, months, ago, fall, 2017, 6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Year  Month                 Speaker  \\\n",
       "0  2019-04-01  2019      4          Ulisses Soares   \n",
       "1  2019-04-01  2019      4            Becky Craven   \n",
       "2  2019-04-01  2019      4          Brook P. Hales   \n",
       "3  2019-04-01  2019      4      Dieter F. Uchtdorf   \n",
       "4  2019-04-01  2019      4  W. Christopher Waddell   \n",
       "\n",
       "                                                Role  \\\n",
       "0               Of the Quorum of the Twelve Apostles   \n",
       "1  Second Counselor in the Young Women General Pr...   \n",
       "2                                     Of the Seventy   \n",
       "3               Of the Quorum of the Twelve Apostles   \n",
       "4        Second Counselor in the Presiding Bishopric   \n",
       "\n",
       "                                            Title  \\\n",
       "0                           How Can I Understand?   \n",
       "1                           Careful versus Casual   \n",
       "2                               Answers to Prayer   \n",
       "3  Missionary Work: Sharing What Is in Your Heart   \n",
       "4                                  Just as He Did   \n",
       "\n",
       "                                                Talk   ID  \\\n",
       "0  My dear brothers and sisters, what a great joy...  196   \n",
       "1  I once saw a sign in a store window that said,...  196   \n",
       "2  An important and comforting doctrine of the go...  196   \n",
       "3  Last month the Twelve were invited by our dear...  196   \n",
       "4  Approximately 18 months ago, in the fall of 20...  196   \n",
       "\n",
       "                                            Sentence  \\\n",
       "0  [My dear brothers and sisters, what a great jo...   \n",
       "1  [I once saw a sign in a store window that said...   \n",
       "2  [An important and comforting doctrine of the g...   \n",
       "3  [Last month the Twelve were invited by our dea...   \n",
       "4  [Approximately 18 months ago, in the fall of 2...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [my, dear, brothers, sisters, great, joy, toge...  \n",
       "1  [i, saw, sign, store, window, said, happiness,...  \n",
       "2  [an, important, comforting, doctrine, gospel, ...  \n",
       "3  [last, month, twelve, invited, dear, prophet, ...  \n",
       "4  [approximately, 18, months, ago, fall, 2017, 6...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['Sentence'] = frame['Talk'].apply(lambda x: sent_tokenize(x))\n",
    "frame['Tokens'] = frame['Talk'].apply(lambda x: tokening(x))\n",
    "frame.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mask = np.array(Image.open(requests.get('http://www.clker.com/cliparts/O/i/x/Y/q/P/yellow-house-hi.png', stream=True).raw))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def generate_wordcloud(words, mask):\n",
    "    word_cloud = WordCloud(width = 512, height = 512, background_color='white', stopwords=STOPWORDS, mask=mask, contour_width=1, contour_color='firebrick').generate(words)\n",
    "    image_colors = ImageColorGenerator(mask)\n",
    "    plt.figure(figsize=(10,8),facecolor = 'white', edgecolor='blue')\n",
    "    plt.imshow(word_cloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "    \n",
    "#Run the following to generate your wordcloud\n",
    "generate_wordcloud(y, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def lemm_tokes(texts):\n",
    "    lems = [wn.lemmatize(text) for text in texts]\n",
    "    return lems\n",
    "\n",
    "def stem_tokes(texts):\n",
    "    lems = [ps.stem(text) for text in texts]\n",
    "    return lems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['Lem_Tokens'] = frame['Tokens'].apply(lambda x: lemm_tokes(x))\n",
    "frame['Stem_Tokens'] = frame['Tokens'].apply(lambda x: stem_tokes(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3839 entries, 0 to 3838\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Date         3839 non-null   object\n",
      " 1   Year         3839 non-null   int64 \n",
      " 2   Month        3839 non-null   int64 \n",
      " 3   Speaker      3839 non-null   object\n",
      " 4   Role         3839 non-null   object\n",
      " 5   Title        3839 non-null   object\n",
      " 6   Talk         3839 non-null   object\n",
      " 7   ID           3839 non-null   int32 \n",
      " 8   Sentence     3839 non-null   object\n",
      " 9   Tokens       3839 non-null   object\n",
      " 10  Lem_Tokens   3839 non-null   object\n",
      " 11  Stem_Tokens  3839 non-null   object\n",
      " 12  POS          3839 non-null   object\n",
      "dtypes: int32(1), int64(2), object(10)\n",
      "memory usage: 375.0+ KB\n"
     ]
    }
   ],
   "source": [
    "frame.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(sentence): \n",
    "  \n",
    "    # Create a SentimentIntensityAnalyzer object. \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    # polarity_scores method of SentimentIntensityAnalyzer \n",
    "    # oject gives a sentiment dictionary. \n",
    "    # which contains pos, neg, neu, and compound scores. \n",
    "    sentiment_dict = analyzer.polarity_scores(sentence) \n",
    "    \n",
    "    # decide sentiment as positive, negative and neutral \n",
    "    if sentiment_dict['compound'] >= 0.05 : \n",
    "        result = \"Positive\"\n",
    "    elif sentiment_dict['compound'] <= - 0.05 : \n",
    "        result = \"Negative\" \n",
    "    else : \n",
    "        result = \"Neutral\"\n",
    "        \n",
    "    return sentiment_dict['compound']\n",
    "\n",
    "def scoring(sents):\n",
    "    scores = []\n",
    "    for i in range(len(sents)):\n",
    "        num = sentiment_scores(sents[i])\n",
    "        scores.append(num)\n",
    "    x = sum(scores)/len(scores)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "frame[\"Sentiment\"] = frame['Sentence'].apply(lambda x: scoring(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the processed titles\n",
    "count_data = count_vectorizer.fit_transform(frame['Talk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=7, n_jobs=-1,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below\n",
    "number_topics = 7\n",
    "number_words = 10\n",
    "# Create and fit the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics found via LDA:\n",
      "\n",
      "Topic #0:\n",
      "church president priesthood presidency quorum members general work lord prophet\n",
      "\n",
      "Topic #1:\n",
      "god lord christ jesus church shall unto holy gospel joseph\n",
      "\n",
      "Topic #2:\n",
      "church lord welfare people work members family poor relief services\n",
      "\n",
      "Topic #3:\n",
      "love church family lord temple young christ women gospel children\n",
      "\n",
      "Topic #4:\n",
      "god lord jesus life christ father unto faith love shall\n",
      "\n",
      "Topic #5:\n",
      "priesthood said young home men time lord father man years\n",
      "\n",
      "Topic #6:\n",
      "children god life family lord love world good time things\n"
     ]
    }
   ],
   "source": [
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calling_group(text):\n",
    "    x = \"NA\"\n",
    "    if re.search('First Presidency', text) or re.search('President of the Church', text):\n",
    "        x = 'First Presidency'\n",
    "    elif re.search('Twelve', text):\n",
    "        x = 'Quorum of the Twelve'\n",
    "    elif re.search('Seventy', text):\n",
    "        x = \"Quorum of the Seventy\"\n",
    "    elif re.search('Primary', text):\n",
    "        x = \"Primary\"\n",
    "    elif re.search('Young Men', text):\n",
    "        x = \"Young Men\"\n",
    "    elif re.search('Young Women', text):\n",
    "        x = \"Young Women\"\n",
    "    elif re.search('Relief', text):\n",
    "        x = \"Relief Society\"\n",
    "    elif re.search('Sunday School', text):\n",
    "        x = \"Sunday School\"\n",
    "    elif re.search('Bishop', text):\n",
    "        x = \"Bishopric\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['GroupedRole'] = frame['Role'].apply(lambda x: calling_group(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n",
    "**********************************************************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(frame['Sentence'][6])\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(doc):\n",
    "    summary = ''\n",
    "    ent_list = []\n",
    "    \n",
    "    for e in doc.ents:\n",
    "        if e.label_ == \"PERSON\":\n",
    "            ent_list.append(str(e).lower().strip())\n",
    "            \n",
    "    ent_list = list( dict.fromkeys(ent_list) )\n",
    "    \n",
    "    for ent in ent_list:\n",
    "        sentences = textacy.extract.semistructured_statements(doc, ent)\n",
    "        for statement in sentences:\n",
    "            subject, verb, fact = statement\n",
    "            if len(fact) > 1:\n",
    "                print(ent)\n",
    "                print(\" - Fact: \", fact)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def print_ents(doc):\n",
    "    sentence_spans = list(doc.sents)\n",
    "    displacy.render(sentence_spans, style=\"ent\")\n",
    "    \n",
    "print_ents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',analyzer='word', max_features=8000)\n",
    "x_counts = vectorizer.fit_transform(frame[\"Talk\"]);\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False);\n",
    "x_tfidf = transformer.fit_transform(x_counts);\n",
    "\n",
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 9\n",
    "\n",
    "#obtain a NMF model.\n",
    "model = NMF(n_components=num_topics, init='nndsvd');\n",
    "#fit the model\n",
    "model.fit(xtfidf_norm)\n",
    "\n",
    "def get_nmf_topics(model):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe = get_nmf_topics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "      <th>Topic # 06</th>\n",
       "      <th>Topic # 07</th>\n",
       "      <th>Topic # 08</th>\n",
       "      <th>Topic # 09</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>na</td>\n",
       "      <td>proposed</td>\n",
       "      <td>god</td>\n",
       "      <td>auditing</td>\n",
       "      <td>emeritus</td>\n",
       "      <td>rededicated</td>\n",
       "      <td>uplifted</td>\n",
       "      <td>auditing</td>\n",
       "      <td>priesthood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hunt</td>\n",
       "      <td>favor</td>\n",
       "      <td>lord</td>\n",
       "      <td>audit</td>\n",
       "      <td>statistical</td>\n",
       "      <td>statistical</td>\n",
       "      <td>constituted</td>\n",
       "      <td>budgets</td>\n",
       "      <td>relief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fiery</td>\n",
       "      <td>manifest</td>\n",
       "      <td>shall</td>\n",
       "      <td>department</td>\n",
       "      <td>branches</td>\n",
       "      <td>31</td>\n",
       "      <td>authorities</td>\n",
       "      <td>certified</td>\n",
       "      <td>society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tutoring</td>\n",
       "      <td>opposed</td>\n",
       "      <td>ye</td>\n",
       "      <td>audits</td>\n",
       "      <td>territories</td>\n",
       "      <td>issued</td>\n",
       "      <td>manifest</td>\n",
       "      <td>assets</td>\n",
       "      <td>women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>griefs</td>\n",
       "      <td>sustain</td>\n",
       "      <td>christ</td>\n",
       "      <td>controlled</td>\n",
       "      <td>statistics</td>\n",
       "      <td>december</td>\n",
       "      <td>favor</td>\n",
       "      <td>policies</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sufferings</td>\n",
       "      <td>counselor</td>\n",
       "      <td>unto</td>\n",
       "      <td>accounting</td>\n",
       "      <td>31</td>\n",
       "      <td>operation</td>\n",
       "      <td>proposed</td>\n",
       "      <td>expenditures</td>\n",
       "      <td>church</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bags</td>\n",
       "      <td>seventies</td>\n",
       "      <td>jesus</td>\n",
       "      <td>expenditures</td>\n",
       "      <td>seventy</td>\n",
       "      <td>mexico</td>\n",
       "      <td>general</td>\n",
       "      <td>departments</td>\n",
       "      <td>young</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>furnace</td>\n",
       "      <td>release</td>\n",
       "      <td>book</td>\n",
       "      <td>departments</td>\n",
       "      <td>wards</td>\n",
       "      <td>status</td>\n",
       "      <td>officers</td>\n",
       "      <td>audits</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sicknesses</td>\n",
       "      <td>vote</td>\n",
       "      <td>ghost</td>\n",
       "      <td>financial</td>\n",
       "      <td>december</td>\n",
       "      <td>dedicated</td>\n",
       "      <td>changes</td>\n",
       "      <td>funds</td>\n",
       "      <td>welfare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>realities</td>\n",
       "      <td>seventy</td>\n",
       "      <td>father</td>\n",
       "      <td>funds</td>\n",
       "      <td>issued</td>\n",
       "      <td>fort</td>\n",
       "      <td>opposed</td>\n",
       "      <td>approved</td>\n",
       "      <td>missionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>praise</td>\n",
       "      <td>quorum</td>\n",
       "      <td>holy</td>\n",
       "      <td>operations</td>\n",
       "      <td>status</td>\n",
       "      <td>argentina</td>\n",
       "      <td>sustain</td>\n",
       "      <td>department</td>\n",
       "      <td>missionaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>arm</td>\n",
       "      <td>russell</td>\n",
       "      <td>mormon</td>\n",
       "      <td>procedures</td>\n",
       "      <td>widow</td>\n",
       "      <td>arizona</td>\n",
       "      <td>sign</td>\n",
       "      <td>accordance</td>\n",
       "      <td>bishop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>preeminent</td>\n",
       "      <td>presidency</td>\n",
       "      <td>thy</td>\n",
       "      <td>finance</td>\n",
       "      <td>reports</td>\n",
       "      <td>report</td>\n",
       "      <td>contrary</td>\n",
       "      <td>safeguarding</td>\n",
       "      <td>president</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>encompassing</td>\n",
       "      <td>robert</td>\n",
       "      <td>life</td>\n",
       "      <td>audited</td>\n",
       "      <td>information</td>\n",
       "      <td>peru</td>\n",
       "      <td>present</td>\n",
       "      <td>auditors</td>\n",
       "      <td>temple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ancestors</td>\n",
       "      <td>sign</td>\n",
       "      <td>thou</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>counselor</td>\n",
       "      <td>regarding</td>\n",
       "      <td>hand</td>\n",
       "      <td>procedures</td>\n",
       "      <td>ward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cloth</td>\n",
       "      <td>richard</td>\n",
       "      <td>light</td>\n",
       "      <td>firms</td>\n",
       "      <td>presidency</td>\n",
       "      <td>utah</td>\n",
       "      <td>conference</td>\n",
       "      <td>expenditure</td>\n",
       "      <td>sisters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>research</td>\n",
       "      <td>presidencies</td>\n",
       "      <td>earth</td>\n",
       "      <td>budget</td>\n",
       "      <td>prior</td>\n",
       "      <td>florida</td>\n",
       "      <td>exception</td>\n",
       "      <td>professionals</td>\n",
       "      <td>quorum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>deceit</td>\n",
       "      <td>boyd</td>\n",
       "      <td>savior</td>\n",
       "      <td>committee</td>\n",
       "      <td>membership</td>\n",
       "      <td>growth</td>\n",
       "      <td>sustained</td>\n",
       "      <td>contributions</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>redeeming</td>\n",
       "      <td>packer</td>\n",
       "      <td>joseph</td>\n",
       "      <td>auditors</td>\n",
       "      <td>general</td>\n",
       "      <td>presidency</td>\n",
       "      <td>counselor</td>\n",
       "      <td>respectfully</td>\n",
       "      <td>mission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>allotted</td>\n",
       "      <td>auxiliary</td>\n",
       "      <td>son</td>\n",
       "      <td>budgeting</td>\n",
       "      <td>elder</td>\n",
       "      <td>information</td>\n",
       "      <td>marion</td>\n",
       "      <td>120</td>\n",
       "      <td>sister</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic # 01    Topic # 02 Topic # 03    Topic # 04   Topic # 05  \\\n",
       "0             na      proposed        god      auditing     emeritus   \n",
       "1           hunt         favor       lord         audit  statistical   \n",
       "2          fiery      manifest      shall    department     branches   \n",
       "3       tutoring       opposed         ye        audits  territories   \n",
       "4         griefs       sustain     christ    controlled   statistics   \n",
       "5     sufferings     counselor       unto    accounting           31   \n",
       "6           bags     seventies      jesus  expenditures      seventy   \n",
       "7        furnace       release       book   departments        wards   \n",
       "8     sicknesses          vote      ghost     financial     december   \n",
       "9      realities       seventy     father         funds       issued   \n",
       "10        praise        quorum       holy    operations       status   \n",
       "11           arm       russell     mormon    procedures        widow   \n",
       "12    preeminent    presidency        thy       finance      reports   \n",
       "13  encompassing        robert       life       audited  information   \n",
       "14     ancestors          sign       thou      reviewed    counselor   \n",
       "15         cloth       richard      light         firms   presidency   \n",
       "16      research  presidencies      earth        budget        prior   \n",
       "17        deceit          boyd     savior     committee   membership   \n",
       "18     redeeming        packer     joseph      auditors      general   \n",
       "19      allotted     auxiliary        son     budgeting        elder   \n",
       "\n",
       "     Topic # 06   Topic # 07     Topic # 08    Topic # 09  \n",
       "0   rededicated     uplifted       auditing    priesthood  \n",
       "1   statistical  constituted        budgets        relief  \n",
       "2            31  authorities      certified       society  \n",
       "3        issued     manifest         assets         women  \n",
       "4      december        favor       policies        family  \n",
       "5     operation     proposed   expenditures        church  \n",
       "6        mexico      general    departments         young  \n",
       "7        status     officers         audits          home  \n",
       "8     dedicated      changes          funds       welfare  \n",
       "9          fort      opposed       approved    missionary  \n",
       "10    argentina      sustain     department  missionaries  \n",
       "11      arizona         sign     accordance        bishop  \n",
       "12       report     contrary   safeguarding     president  \n",
       "13         peru      present       auditors        temple  \n",
       "14    regarding         hand     procedures          ward  \n",
       "15         utah   conference    expenditure       sisters  \n",
       "16      florida    exception  professionals        quorum  \n",
       "17       growth    sustained  contributions          love  \n",
       "18   presidency    counselor   respectfully       mission  \n",
       "19  information       marion            120        sister  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
